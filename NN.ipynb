{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCKBz8twGz8r"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzJ9yTnDHBPX",
        "outputId": "caea6fe0-9826-4a6d-99e8-2ff6150de9eb"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwFEjobvHGyE"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/Datasets /archive (23).zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9aejgLZHV5T"
      },
      "source": [
        "df = pd.read_csv('/content/Admission_Predict.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMafRd7HHYiV"
      },
      "source": [
        "pd.set_option('display.max_rows',400)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zlxsq1FHbKj"
      },
      "source": [
        "np.random.seed(10)\n",
        "\n",
        "n_sample = df.shape[0]\n",
        "shuffled_indices = np.random.permutation(n_sample)\n",
        "#print(shuffled_indices)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJMO5-VIHyYF"
      },
      "source": [
        "features = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n",
        "       'LOR ', 'CGPA', 'Chance of Admit ']\n",
        "\n",
        "x = np.array(df[features])\n",
        "y = np.array(df['Research'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djbNyeiUIYiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8aeb9fc-ee5c-47a7-9776-4e30a9f2fb8b"
      },
      "source": [
        "n_val =int( 0.1 * df.shape[0])\n",
        "print(n_val)\n",
        "\n",
        "train_indices = shuffled_indices[n_val:]\n",
        "test_indices = shuffled_indices[:n_val]\n",
        "\n",
        "x_train = x[train_indices]\n",
        "x_test  = x[test_indices]\n",
        "\n",
        "y_train = y[train_indices]\n",
        "y_test  = y[test_indices]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9XOjUy4J3MR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574e0a28-bd6b-4cfa-f7ae-2bfd7eff3ce0"
      },
      "source": [
        "x_train"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[332.  , 116.  ,   5.  , ...,   5.  ,   9.28,   0.94],\n",
              "       [326.  , 112.  ,   4.  , ...,   3.5 ,   9.12,   0.84],\n",
              "       [327.  , 114.  ,   3.  , ...,   3.  ,   9.02,   0.61],\n",
              "       ...,\n",
              "       [314.  , 105.  ,   3.  , ...,   2.5 ,   8.3 ,   0.54],\n",
              "       [300.  , 100.  ,   3.  , ...,   3.  ,   8.66,   0.64],\n",
              "       [313.  , 102.  ,   3.  , ...,   2.5 ,   8.68,   0.71]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG2WcgnuIv4L"
      },
      "source": [
        "x_train = (x_train - x_train.mean(axis = 0))/x_train.std(axis = 0)\n",
        "x_test = (x_test - x_test.mean(axis = 0))/x_test.std(axis = 0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkdl4ffxJkrW"
      },
      "source": [
        "x_train = x_train.reshape(7,-1)\n",
        "y_train = y_train.reshape(1,-1)\n",
        "x_test = x_test.reshape(7,-1)\n",
        "y_test = y_test.reshape(1,-1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2BUw9AUKORw"
      },
      "source": [
        "class NN:\n",
        "\n",
        "  def __init__(self,input_size,h1_size,output_size):\n",
        "    #-----Inputs are assigned to the model----#\n",
        "    self.input_size = input_size\n",
        "    self.h1_size = h1_size \n",
        "    self.output_size = output_size \n",
        "    \n",
        "    #----Initiailze the weights----#\n",
        "    self.W1 = np.random.uniform(low = -1,high = 1, size = (self.h1_size,self.input_size))\n",
        "    self.b1 = np.zeros([self.h1_size,1])\n",
        "    self.W2 = np.random.uniform(low = -1,high = 1, size = (self.output_size,self.h1_size))\n",
        "    self.b2 = np.zeros([self.output_size,1])\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.Z1 = np.dot(self.W1,x) + self.b1\n",
        "    self.A1 = self.Tanh(self.Z1)\n",
        "\n",
        "    self.Z2 = np.dot(self.W2,self.A1) + self.b2\n",
        "    self.A2 = self.Sigmoid(self.Z2)\n",
        "    return self.A2\n",
        "\n",
        "  def NLLloss(self,output,y):\n",
        "    loss = -np.sum((np.dot(y,np.log(self.A2).T) + np.dot(1-y,np.log(1-self.A2).T))/x.shape[1])\n",
        "    return loss\n",
        "\n",
        "  def backward(self,x,y):\n",
        "    self.dA2 = (self.A2-y)/np.multiply(self.A2,1-self.A2)\n",
        "    self.dZ2 = self.dA2*self.Sigmoid(self.Z2,derivative = True)\n",
        "    self.dW2 = np.dot(self.dZ2,self.A1.T)/x.shape[1]\n",
        "    self.db2 = np.sum(self.dZ2, axis = 1 , keepdims = True)/x.shape[1]\n",
        "\n",
        "    self.dA1 = np.dot(self.W2.T,self.dZ2)\n",
        "    self.dZ1 = self.dA1*self.Tanh(self.Z1, derivative = True)\n",
        "    self.dW1 = np.dot(self.dZ1,x.T)/x.shape[1]\n",
        "    self.db1 = np.sum(self.dZ1 , axis = 1 , keepdims = True)/x.shape[1]\n",
        "\n",
        "  #-----Define various activation functions-----#\n",
        "  def Sigmoid(self,x,derivative = False):\n",
        "    if derivative == False:\n",
        "      x = 1/(1 + np.exp(-x))\n",
        "      return x\n",
        "    else:\n",
        "      y = 1/(1 + np.exp(-x))\n",
        "      return y*(1-y)\n",
        "\n",
        "  def Tanh(self,x,derivative = False):\n",
        "    if derivative == False:\n",
        "      x = np.tanh(x)\n",
        "      return x\n",
        "    else:\n",
        "      y = np.tanh(x)\n",
        "      return 1-y**2"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM2hq6enJmwE"
      },
      "source": [
        "model = NN(input_size=7,h1_size=64,output_size=1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyEOq4suJ0Ym"
      },
      "source": [
        "outputs_train = model.forward(x_train)\n",
        "loss = model.NLLloss(outputs,y_train)\n",
        "model.backward(x_train,y_train)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihlaHTMTKnRr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d133dd-5ddb-49e6-d908-842cc3e0c340"
      },
      "source": [
        "#------Update the parameters using Gradient Descent------#\n",
        "lr = 1e-1\n",
        "num_epochs = 500\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "  outputs = model.forward(x_train)\n",
        "  loss = model.NLLloss(outputs,y_train)\n",
        "  model.backward(x_train,y_train)\n",
        "  #------Update the parameters------#\n",
        "  model.W1 -= lr*model.dW1\n",
        "  model.b1 -= lr*model.db1\n",
        "  model.W2 -= lr*model.dW2\n",
        "  model.b2 -= lr*model.db2\n",
        "  #-----Print the loss-----#\n",
        "  print(\"[{}/{}] Loss: {}\".format(epochs+1,num_epochs,loss))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/500] Loss: 3.6351849315180234\n",
            "[2/500] Loss: 3.6339057973158626\n",
            "[3/500] Loss: 3.6326273215869915\n",
            "[4/500] Loss: 3.6313495039434796\n",
            "[5/500] Loss: 3.630072343997607\n",
            "[6/500] Loss: 3.628795841361852\n",
            "[7/500] Loss: 3.6275199956489033\n",
            "[8/500] Loss: 3.6262448064716466\n",
            "[9/500] Loss: 3.624970273443178\n",
            "[10/500] Loss: 3.623696396176793\n",
            "[11/500] Loss: 3.6224231742859936\n",
            "[12/500] Loss: 3.6211506073844864\n",
            "[13/500] Loss: 3.6198786950861837\n",
            "[14/500] Loss: 3.6186074370051946\n",
            "[15/500] Loss: 3.6173368327558424\n",
            "[16/500] Loss: 3.616066881952645\n",
            "[17/500] Loss: 3.6147975842103324\n",
            "[18/500] Loss: 3.613528939143832\n",
            "[19/500] Loss: 3.6122609463682784\n",
            "[20/500] Loss: 3.610993605499013\n",
            "[21/500] Loss: 3.6097269161515726\n",
            "[22/500] Loss: 3.6084608779417087\n",
            "[23/500] Loss: 3.6071954904853643\n",
            "[24/500] Loss: 3.6059307533987\n",
            "[25/500] Loss: 3.60466666629807\n",
            "[26/500] Loss: 3.6034032288000364\n",
            "[27/500] Loss: 3.602140440521364\n",
            "[28/500] Loss: 3.6008783010790233\n",
            "[29/500] Loss: 3.5996168100901857\n",
            "[30/500] Loss: 3.5983559671722287\n",
            "[31/500] Loss: 3.597095771942731\n",
            "[32/500] Loss: 3.5958362240194788\n",
            "[33/500] Loss: 3.5945773230204567\n",
            "[34/500] Loss: 3.593319068563859\n",
            "[35/500] Loss: 3.5920614602680807\n",
            "[36/500] Loss: 3.590804497751717\n",
            "[37/500] Loss: 3.589548180633574\n",
            "[38/500] Loss: 3.5882925085326534\n",
            "[39/500] Loss: 3.5870374810681676\n",
            "[40/500] Loss: 3.585783097859528\n",
            "[41/500] Loss: 3.58452935852635\n",
            "[42/500] Loss: 3.5832762626884547\n",
            "[43/500] Loss: 3.582023809965865\n",
            "[44/500] Loss: 3.5807719999788055\n",
            "[45/500] Loss: 3.5795208323477077\n",
            "[46/500] Loss: 3.5782703066932053\n",
            "[47/500] Loss: 3.5770204226361337\n",
            "[48/500] Loss: 3.5757711797975333\n",
            "[49/500] Loss: 3.5745225777986467\n",
            "[50/500] Loss: 3.5732746162609184\n",
            "[51/500] Loss: 3.5720272948060026\n",
            "[52/500] Loss: 3.570780613055749\n",
            "[53/500] Loss: 3.569534570632212\n",
            "[54/500] Loss: 3.568289167157656\n",
            "[55/500] Loss: 3.567044402254539\n",
            "[56/500] Loss: 3.5658002755455267\n",
            "[57/500] Loss: 3.564556786653488\n",
            "[58/500] Loss: 3.5633139352014958\n",
            "[59/500] Loss: 3.562071720812822\n",
            "[60/500] Loss: 3.560830143110946\n",
            "[61/500] Loss: 3.559589201719547\n",
            "[62/500] Loss: 3.558348896262509\n",
            "[63/500] Loss: 3.557109226363918\n",
            "[64/500] Loss: 3.5558701916480624\n",
            "[65/500] Loss: 3.554631791739436\n",
            "[66/500] Loss: 3.5533940262627324\n",
            "[67/500] Loss: 3.552156894842846\n",
            "[68/500] Loss: 3.550920397104883\n",
            "[69/500] Loss: 3.549684532674142\n",
            "[70/500] Loss: 3.5484493011761313\n",
            "[71/500] Loss: 3.5472147022365563\n",
            "[72/500] Loss: 3.5459807354813306\n",
            "[73/500] Loss: 3.5447474005365676\n",
            "[74/500] Loss: 3.543514697028581\n",
            "[75/500] Loss: 3.5422826245838936\n",
            "[76/500] Loss: 3.541051182829223\n",
            "[77/500] Loss: 3.5398203713914933\n",
            "[78/500] Loss: 3.538590189897834\n",
            "[79/500] Loss: 3.5373606379755698\n",
            "[80/500] Loss: 3.5361317152522345\n",
            "[81/500] Loss: 3.5349034213555592\n",
            "[82/500] Loss: 3.5336757559134804\n",
            "[83/500] Loss: 3.5324487185541367\n",
            "[84/500] Loss: 3.5312223089058676\n",
            "[85/500] Loss: 3.529996526597216\n",
            "[86/500] Loss: 3.5287713712569277\n",
            "[87/500] Loss: 3.527546842513948\n",
            "[88/500] Loss: 3.5263229399974247\n",
            "[89/500] Loss: 3.525099663336712\n",
            "[90/500] Loss: 3.523877012161361\n",
            "[91/500] Loss: 3.5226549861011276\n",
            "[92/500] Loss: 3.521433584785968\n",
            "[93/500] Loss: 3.5202128078460433\n",
            "[94/500] Loss: 3.518992654911713\n",
            "[95/500] Loss: 3.5177731256135414\n",
            "[96/500] Loss: 3.516554219582292\n",
            "[97/500] Loss: 3.5153359364489325\n",
            "[98/500] Loss: 3.5141182758446314\n",
            "[99/500] Loss: 3.5129012374007593\n",
            "[100/500] Loss: 3.511684820748887\n",
            "[101/500] Loss: 3.5104690255207904\n",
            "[102/500] Loss: 3.509253851348442\n",
            "[103/500] Loss: 3.5080392978640202\n",
            "[104/500] Loss: 3.5068253646999046\n",
            "[105/500] Loss: 3.5056120514886735\n",
            "[106/500] Loss: 3.5043993578631136\n",
            "[107/500] Loss: 3.5031872834562003\n",
            "[108/500] Loss: 3.5019758279011244\n",
            "[109/500] Loss: 3.5007649908312692\n",
            "[110/500] Loss: 3.499554771880224\n",
            "[111/500] Loss: 3.4983451706817754\n",
            "[112/500] Loss: 3.497136186869914\n",
            "[113/500] Loss: 3.4959278200788337\n",
            "[114/500] Loss: 3.494720069942924\n",
            "[115/500] Loss: 3.493512936096781\n",
            "[116/500] Loss: 3.4923064181751977\n",
            "[117/500] Loss: 3.4911005158131707\n",
            "[118/500] Loss: 3.4898952286459015\n",
            "[119/500] Loss: 3.4886905563087813\n",
            "[120/500] Loss: 3.487486498437415\n",
            "[121/500] Loss: 3.4862830546676\n",
            "[122/500] Loss: 3.4850802246353374\n",
            "[123/500] Loss: 3.4838780079768314\n",
            "[124/500] Loss: 3.482676404328484\n",
            "[125/500] Loss: 3.4814754133268977\n",
            "[126/500] Loss: 3.480275034608877\n",
            "[127/500] Loss: 3.479075267811431\n",
            "[128/500] Loss: 3.4778761125717628\n",
            "[129/500] Loss: 3.476677568527278\n",
            "[130/500] Loss: 3.4754796353155863\n",
            "[131/500] Loss: 3.474282312574495\n",
            "[132/500] Loss: 3.4730855999420123\n",
            "[133/500] Loss: 3.4718894970563463\n",
            "[134/500] Loss: 3.470694003555908\n",
            "[135/500] Loss: 3.4694991190793067\n",
            "[136/500] Loss: 3.4683048432653516\n",
            "[137/500] Loss: 3.467111175753056\n",
            "[138/500] Loss: 3.465918116181629\n",
            "[139/500] Loss: 3.464725664190481\n",
            "[140/500] Loss: 3.4635338194192262\n",
            "[141/500] Loss: 3.462342581507673\n",
            "[142/500] Loss: 3.4611519500958368\n",
            "[143/500] Loss: 3.459961924823926\n",
            "[144/500] Loss: 3.4587725053323544\n",
            "[145/500] Loss: 3.4575836912617346\n",
            "[146/500] Loss: 3.4563954822528777\n",
            "[147/500] Loss: 3.4552078779467945\n",
            "[148/500] Loss: 3.4540208779846986\n",
            "[149/500] Loss: 3.452834482008001\n",
            "[150/500] Loss: 3.451648689658313\n",
            "[151/500] Loss: 3.4504635005774467\n",
            "[152/500] Loss: 3.449278914407412\n",
            "[153/500] Loss: 3.4480949307904223\n",
            "[154/500] Loss: 3.4469115493688838\n",
            "[155/500] Loss: 3.445728769785409\n",
            "[156/500] Loss: 3.444546591682806\n",
            "[157/500] Loss: 3.4433650147040873\n",
            "[158/500] Loss: 3.4421840384924565\n",
            "[159/500] Loss: 3.4410036626913234\n",
            "[160/500] Loss: 3.4398238869442994\n",
            "[161/500] Loss: 3.438644710895184\n",
            "[162/500] Loss: 3.437466134187989\n",
            "[163/500] Loss: 3.436288156466916\n",
            "[164/500] Loss: 3.435110777376372\n",
            "[165/500] Loss: 3.4339339965609597\n",
            "[166/500] Loss: 3.4327578136654813\n",
            "[167/500] Loss: 3.431582228334938\n",
            "[168/500] Loss: 3.430407240214532\n",
            "[169/500] Loss: 3.429232848949663\n",
            "[170/500] Loss: 3.4280590541859293\n",
            "[171/500] Loss: 3.4268858555691293\n",
            "[172/500] Loss: 3.425713252745259\n",
            "[173/500] Loss: 3.4245412453605124\n",
            "[174/500] Loss: 3.423369833061288\n",
            "[175/500] Loss: 3.4221990154941744\n",
            "[176/500] Loss: 3.4210287923059632\n",
            "[177/500] Loss: 3.4198591631436464\n",
            "[178/500] Loss: 3.4186901276544117\n",
            "[179/500] Loss: 3.4175216854856467\n",
            "[180/500] Loss: 3.4163538362849386\n",
            "[181/500] Loss: 3.4151865797000687\n",
            "[182/500] Loss: 3.4140199153790207\n",
            "[183/500] Loss: 3.412853842969976\n",
            "[184/500] Loss: 3.4116883621213137\n",
            "[185/500] Loss: 3.4105234724816103\n",
            "[186/500] Loss: 3.409359173699643\n",
            "[187/500] Loss: 3.4081954654243845\n",
            "[188/500] Loss: 3.4070323473050084\n",
            "[189/500] Loss: 3.405869818990882\n",
            "[190/500] Loss: 3.404707880131575\n",
            "[191/500] Loss: 3.4035465303768517\n",
            "[192/500] Loss: 3.4023857693766795\n",
            "[193/500] Loss: 3.401225596781217\n",
            "[194/500] Loss: 3.400066012240827\n",
            "[195/500] Loss: 3.398907015406063\n",
            "[196/500] Loss: 3.3977486059276814\n",
            "[197/500] Loss: 3.396590783456637\n",
            "[198/500] Loss: 3.395433547644078\n",
            "[199/500] Loss: 3.3942768981413534\n",
            "[200/500] Loss: 3.393120834600007\n",
            "[201/500] Loss: 3.391965356671785\n",
            "[202/500] Loss: 3.3908104640086245\n",
            "[203/500] Loss: 3.389656156262667\n",
            "[204/500] Loss: 3.388502433086244\n",
            "[205/500] Loss: 3.3873492941318903\n",
            "[206/500] Loss: 3.386196739052334\n",
            "[207/500] Loss: 3.3850447675005038\n",
            "[208/500] Loss: 3.383893379129522\n",
            "[209/500] Loss: 3.38274257359271\n",
            "[210/500] Loss: 3.381592350543585\n",
            "[211/500] Loss: 3.380442709635865\n",
            "[212/500] Loss: 3.3792936505234605\n",
            "[213/500] Loss: 3.3781451728604805\n",
            "[214/500] Loss: 3.376997276301229\n",
            "[215/500] Loss: 3.3758499605002106\n",
            "[216/500] Loss: 3.3747032251121234\n",
            "[217/500] Loss: 3.3735570697918655\n",
            "[218/500] Loss: 3.372411494194526\n",
            "[219/500] Loss: 3.3712664979753972\n",
            "[220/500] Loss: 3.3701220807899626\n",
            "[221/500] Loss: 3.368978242293905\n",
            "[222/500] Loss: 3.3678349821431013\n",
            "[223/500] Loss: 3.3666922999936304\n",
            "[224/500] Loss: 3.3655501955017613\n",
            "[225/500] Loss: 3.36440866832396\n",
            "[226/500] Loss: 3.363267718116892\n",
            "[227/500] Loss: 3.3621273445374165\n",
            "[228/500] Loss: 3.36098754724259\n",
            "[229/500] Loss: 3.359848325889664\n",
            "[230/500] Loss: 3.3587096801360863\n",
            "[231/500] Loss: 3.3575716096395007\n",
            "[232/500] Loss: 3.3564341140577474\n",
            "[233/500] Loss: 3.3552971930488624\n",
            "[234/500] Loss: 3.354160846271076\n",
            "[235/500] Loss: 3.3530250733828177\n",
            "[236/500] Loss: 3.351889874042708\n",
            "[237/500] Loss: 3.3507552479095666\n",
            "[238/500] Loss: 3.349621194642407\n",
            "[239/500] Loss: 3.3484877139004388\n",
            "[240/500] Loss: 3.347354805343067\n",
            "[241/500] Loss: 3.346222468629892\n",
            "[242/500] Loss: 3.34509070342071\n",
            "[243/500] Loss: 3.3439595093755123\n",
            "[244/500] Loss: 3.342828886154485\n",
            "[245/500] Loss: 3.3416988334180084\n",
            "[246/500] Loss: 3.3405693508266614\n",
            "[247/500] Loss: 3.3394404380412133\n",
            "[248/500] Loss: 3.338312094722634\n",
            "[249/500] Loss: 3.3371843205320815\n",
            "[250/500] Loss: 3.336057115130914\n",
            "[251/500] Loss: 3.3349304781806848\n",
            "[252/500] Loss: 3.3338044093431387\n",
            "[253/500] Loss: 3.332678908280216\n",
            "[254/500] Loss: 3.3315539746540526\n",
            "[255/500] Loss: 3.330429608126981\n",
            "[256/500] Loss: 3.3293058083615223\n",
            "[257/500] Loss: 3.3281825750203997\n",
            "[258/500] Loss: 3.3270599077665253\n",
            "[259/500] Loss: 3.325937806263007\n",
            "[260/500] Loss: 3.3248162701731494\n",
            "[261/500] Loss: 3.323695299160449\n",
            "[262/500] Loss: 3.322574892888596\n",
            "[263/500] Loss: 3.3214550510214758\n",
            "[264/500] Loss: 3.32033577322317\n",
            "[265/500] Loss: 3.3192170591579497\n",
            "[266/500] Loss: 3.3180989084902826\n",
            "[267/500] Loss: 3.316981320884832\n",
            "[268/500] Loss: 3.3158642960064526\n",
            "[269/500] Loss: 3.3147478335201948\n",
            "[270/500] Loss: 3.313631933091299\n",
            "[271/500] Loss: 3.3125165943852033\n",
            "[272/500] Loss: 3.31140181706754\n",
            "[273/500] Loss: 3.3102876008041315\n",
            "[274/500] Loss: 3.309173945260995\n",
            "[275/500] Loss: 3.3080608501043423\n",
            "[276/500] Loss: 3.3069483150005783\n",
            "[277/500] Loss: 3.3058363396163\n",
            "[278/500] Loss: 3.3047249236183\n",
            "[279/500] Loss: 3.3036140666735627\n",
            "[280/500] Loss: 3.302503768449264\n",
            "[281/500] Loss: 3.3013940286127768\n",
            "[282/500] Loss: 3.300284846831665\n",
            "[283/500] Loss: 3.2991762227736836\n",
            "[284/500] Loss: 3.298068156106784\n",
            "[285/500] Loss: 3.296960646499109\n",
            "[286/500] Loss: 3.295853693618994\n",
            "[287/500] Loss: 3.294747297134966\n",
            "[288/500] Loss: 3.29364145671575\n",
            "[289/500] Loss: 3.2925361720302564\n",
            "[290/500] Loss: 3.2914314427475935\n",
            "[291/500] Loss: 3.290327268537058\n",
            "[292/500] Loss: 3.2892236490681444\n",
            "[293/500] Loss: 3.2881205840105343\n",
            "[294/500] Loss: 3.287018073034105\n",
            "[295/500] Loss: 3.2859161158089236\n",
            "[296/500] Loss: 3.284814712005254\n",
            "[297/500] Loss: 3.283713861293547\n",
            "[298/500] Loss: 3.282613563344448\n",
            "[299/500] Loss: 3.2815138178287944\n",
            "[300/500] Loss: 3.2804146244176144\n",
            "[301/500] Loss: 3.27931598278213\n",
            "[302/500] Loss: 3.2782178925937546\n",
            "[303/500] Loss: 3.2771203535240927\n",
            "[304/500] Loss: 3.276023365244941\n",
            "[305/500] Loss: 3.274926927428286\n",
            "[306/500] Loss: 3.2738310397463097\n",
            "[307/500] Loss: 3.2727357018713823\n",
            "[308/500] Loss: 3.2716409134760673\n",
            "[309/500] Loss: 3.2705466742331204\n",
            "[310/500] Loss: 3.2694529838154844\n",
            "[311/500] Loss: 3.268359841896299\n",
            "[312/500] Loss: 3.2672672481488925\n",
            "[313/500] Loss: 3.266175202246782\n",
            "[314/500] Loss: 3.265083703863681\n",
            "[315/500] Loss: 3.26399275267349\n",
            "[316/500] Loss: 3.262902348350301\n",
            "[317/500] Loss: 3.2618124905684\n",
            "[318/500] Loss: 3.2607231790022593\n",
            "[319/500] Loss: 3.259634413326547\n",
            "[320/500] Loss: 3.2585461932161146\n",
            "[321/500] Loss: 3.2574585183460143\n",
            "[322/500] Loss: 3.2563713883914795\n",
            "[323/500] Loss: 3.255284803027942\n",
            "[324/500] Loss: 3.254198761931018\n",
            "[325/500] Loss: 3.253113264776516\n",
            "[326/500] Loss: 3.252028311240438\n",
            "[327/500] Loss: 3.250943900998972\n",
            "[328/500] Loss: 3.249860033728497\n",
            "[329/500] Loss: 3.2487767091055857\n",
            "[330/500] Loss: 3.2476939268069978\n",
            "[331/500] Loss: 3.246611686509681\n",
            "[332/500] Loss: 3.245529987890779\n",
            "[333/500] Loss: 3.2444488306276194\n",
            "[334/500] Loss: 3.2433682143977256\n",
            "[335/500] Loss: 3.2422881388788043\n",
            "[336/500] Loss: 3.2412086037487575\n",
            "[337/500] Loss: 3.240129608685673\n",
            "[338/500] Loss: 3.239051153367831\n",
            "[339/500] Loss: 3.2379732374736974\n",
            "[340/500] Loss: 3.2368958606819342\n",
            "[341/500] Loss: 3.235819022671386\n",
            "[342/500] Loss: 3.234742723121091\n",
            "[343/500] Loss: 3.233666961710274\n",
            "[344/500] Loss: 3.232591738118351\n",
            "[345/500] Loss: 3.231517052024927\n",
            "[346/500] Loss: 3.2304429031097928\n",
            "[347/500] Loss: 3.2293692910529352\n",
            "[348/500] Loss: 3.2282962155345216\n",
            "[349/500] Loss: 3.2272236762349147\n",
            "[350/500] Loss: 3.226151672834663\n",
            "[351/500] Loss: 3.225080205014504\n",
            "[352/500] Loss: 3.2240092724553664\n",
            "[353/500] Loss: 3.2229388748383627\n",
            "[354/500] Loss: 3.2218690118447983\n",
            "[355/500] Loss: 3.2207996831561654\n",
            "[356/500] Loss: 3.2197308884541442\n",
            "[357/500] Loss: 3.218662627420605\n",
            "[358/500] Loss: 3.217594899737603\n",
            "[359/500] Loss: 3.216527705087387\n",
            "[360/500] Loss: 3.215461043152387\n",
            "[361/500] Loss: 3.214394913615228\n",
            "[362/500] Loss: 3.213329316158718\n",
            "[363/500] Loss: 3.2122642504658563\n",
            "[364/500] Loss: 3.2111997162198276\n",
            "[365/500] Loss: 3.2101357131040067\n",
            "[366/500] Loss: 3.209072240801954\n",
            "[367/500] Loss: 3.208009298997417\n",
            "[368/500] Loss: 3.206946887374334\n",
            "[369/500] Loss: 3.2058850056168313\n",
            "[370/500] Loss: 3.2048236534092176\n",
            "[371/500] Loss: 3.203762830435992\n",
            "[372/500] Loss: 3.2027025363818433\n",
            "[373/500] Loss: 3.201642770931644\n",
            "[374/500] Loss: 3.2005835337704522\n",
            "[375/500] Loss: 3.1995248245835213\n",
            "[376/500] Loss: 3.198466643056283\n",
            "[377/500] Loss: 3.1974089888743618\n",
            "[378/500] Loss: 3.196351861723565\n",
            "[379/500] Loss: 3.1952952612898886\n",
            "[380/500] Loss: 3.1942391872595164\n",
            "[381/500] Loss: 3.1931836393188178\n",
            "[382/500] Loss: 3.192128617154348\n",
            "[383/500] Loss: 3.1910741204528508\n",
            "[384/500] Loss: 3.1900201489012567\n",
            "[385/500] Loss: 3.1889667021866797\n",
            "[386/500] Loss: 3.187913779996422\n",
            "[387/500] Loss: 3.1868613820179736\n",
            "[388/500] Loss: 3.1858095079390085\n",
            "[389/500] Loss: 3.1847581574473875\n",
            "[390/500] Loss: 3.1837073302311594\n",
            "[391/500] Loss: 3.1826570259785547\n",
            "[392/500] Loss: 3.1816072443779952\n",
            "[393/500] Loss: 3.1805579851180843\n",
            "[394/500] Loss: 3.179509247887612\n",
            "[395/500] Loss: 3.1784610323755578\n",
            "[396/500] Loss: 3.17741333827108\n",
            "[397/500] Loss: 3.1763661652635315\n",
            "[398/500] Loss: 3.1753195130424436\n",
            "[399/500] Loss: 3.1742733812975357\n",
            "[400/500] Loss: 3.1732277697187103\n",
            "[401/500] Loss: 3.17218267799606\n",
            "[402/500] Loss: 3.171138105819859\n",
            "[403/500] Loss: 3.170094052880567\n",
            "[404/500] Loss: 3.16905051886883\n",
            "[405/500] Loss: 3.1680075034754784\n",
            "[406/500] Loss: 3.1669650063915262\n",
            "[407/500] Loss: 3.1659230273081778\n",
            "[408/500] Loss: 3.1648815659168155\n",
            "[409/500] Loss: 3.1638406219090087\n",
            "[410/500] Loss: 3.1628001949765157\n",
            "[411/500] Loss: 3.161760284811272\n",
            "[412/500] Loss: 3.160720891105405\n",
            "[413/500] Loss: 3.1596820135512202\n",
            "[414/500] Loss: 3.158643651841213\n",
            "[415/500] Loss: 3.15760580566806\n",
            "[416/500] Loss: 3.1565684747246237\n",
            "[417/500] Loss: 3.155531658703947\n",
            "[418/500] Loss: 3.1544953572992633\n",
            "[419/500] Loss: 3.1534595702039843\n",
            "[420/500] Loss: 3.1524242971117085\n",
            "[421/500] Loss: 3.151389537716219\n",
            "[422/500] Loss: 3.150355291711481\n",
            "[423/500] Loss: 3.1493215587916445\n",
            "[424/500] Loss: 3.148288338651043\n",
            "[425/500] Loss: 3.147255630984193\n",
            "[426/500] Loss: 3.146223435485795\n",
            "[427/500] Loss: 3.1451917518507315\n",
            "[428/500] Loss: 3.144160579774074\n",
            "[429/500] Loss: 3.1431299189510704\n",
            "[430/500] Loss: 3.142099769077156\n",
            "[431/500] Loss: 3.1410701298479458\n",
            "[432/500] Loss: 3.1400410009592434\n",
            "[433/500] Loss: 3.139012382107032\n",
            "[434/500] Loss: 3.1379842729874765\n",
            "[435/500] Loss: 3.1369566732969276\n",
            "[436/500] Loss: 3.1359295827319165\n",
            "[437/500] Loss: 3.13490300098916\n",
            "[438/500] Loss: 3.133876927765555\n",
            "[439/500] Loss: 3.13285136275818\n",
            "[440/500] Loss: 3.1318263056643016\n",
            "[441/500] Loss: 3.1308017561813632\n",
            "[442/500] Loss: 3.1297777140069942\n",
            "[443/500] Loss: 3.1287541788390025\n",
            "[444/500] Loss: 3.1277311503753817\n",
            "[445/500] Loss: 3.1267086283143053\n",
            "[446/500] Loss: 3.125686612354133\n",
            "[447/500] Loss: 3.124665102193402\n",
            "[448/500] Loss: 3.1236440975308315\n",
            "[449/500] Loss: 3.122623598065327\n",
            "[450/500] Loss: 3.1216036034959704\n",
            "[451/500] Loss: 3.1205841135220314\n",
            "[452/500] Loss: 3.119565127842955\n",
            "[453/500] Loss: 3.118546646158372\n",
            "[454/500] Loss: 3.1175286681680916\n",
            "[455/500] Loss: 3.1165111935721086\n",
            "[456/500] Loss: 3.115494222070597\n",
            "[457/500] Loss: 3.1144777533639103\n",
            "[458/500] Loss: 3.113461787152584\n",
            "[459/500] Loss: 3.1124463231373403\n",
            "[460/500] Loss: 3.1114313610190742\n",
            "[461/500] Loss: 3.110416900498866\n",
            "[462/500] Loss: 3.1094029412779767\n",
            "[463/500] Loss: 3.108389483057848\n",
            "[464/500] Loss: 3.1073765255401002\n",
            "[465/500] Loss: 3.1063640684265397\n",
            "[466/500] Loss: 3.105352111419148\n",
            "[467/500] Loss: 3.1043406542200884\n",
            "[468/500] Loss: 3.1033296965317065\n",
            "[469/500] Loss: 3.1023192380565296\n",
            "[470/500] Loss: 3.101309278497259\n",
            "[471/500] Loss: 3.1002998175567824\n",
            "[472/500] Loss: 3.099290854938167\n",
            "[473/500] Loss: 3.0982823903446564\n",
            "[474/500] Loss: 3.097274423479679\n",
            "[475/500] Loss: 3.0962669540468384\n",
            "[476/500] Loss: 3.0952599817499213\n",
            "[477/500] Loss: 3.094253506292893\n",
            "[478/500] Loss: 3.0932475273799\n",
            "[479/500] Loss: 3.0922420447152663\n",
            "[480/500] Loss: 3.0912370580034962\n",
            "[481/500] Loss: 3.090232566949275\n",
            "[482/500] Loss: 3.0892285712574656\n",
            "[483/500] Loss: 3.088225070633112\n",
            "[484/500] Loss: 3.0872220647814337\n",
            "[485/500] Loss: 3.0862195534078345\n",
            "[486/500] Loss: 3.0852175362178955\n",
            "[487/500] Loss: 3.0842160129173726\n",
            "[488/500] Loss: 3.0832149832122084\n",
            "[489/500] Loss: 3.082214446808518\n",
            "[490/500] Loss: 3.0812144034125986\n",
            "[491/500] Loss: 3.080214852730925\n",
            "[492/500] Loss: 3.0792157944701506\n",
            "[493/500] Loss: 3.078217228337107\n",
            "[494/500] Loss: 3.077219154038808\n",
            "[495/500] Loss: 3.0762215712824394\n",
            "[496/500] Loss: 3.0752244797753705\n",
            "[497/500] Loss: 3.0742278792251474\n",
            "[498/500] Loss: 3.0732317693394924\n",
            "[499/500] Loss: 3.0722361498263107\n",
            "[500/500] Loss: 3.071241020393679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXaQdizcMz3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ffeae7-91b8-413b-d16b-1c88a59c1937"
      },
      "source": [
        "#----Model Accuracy on Test Dataset----#\n",
        "outputs_test = model.forward(x_test)\n",
        "loss = model.NLLloss(outputs,y_test)\n",
        "print(loss)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.021675127293685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcfZgM_3IQAL",
        "outputId": "397954f4-5752-4bc8-8261-eac65b958a76"
      },
      "source": [
        "outputs_test"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.205389  , 0.77137199, 0.73165637, 0.46471751, 0.42638496,\n",
              "        0.00268187, 0.51635608, 0.12607124, 0.56083835, 0.05177593,\n",
              "        0.41071092, 0.24273937, 0.06309108, 0.02199178, 0.12063565,\n",
              "        0.00176156, 0.09482397, 0.58843474, 0.02277412, 0.2056326 ,\n",
              "        0.82797546, 0.82709707, 0.92390927, 0.25769992, 0.80483681,\n",
              "        0.9949691 , 0.16407127, 0.23773823, 0.67651536, 0.96567751,\n",
              "        0.04871298, 0.71103873, 0.83025455, 0.84143262, 0.76592178,\n",
              "        0.74334647, 0.39340191, 0.92991645, 0.9787754 , 0.07628153]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3t5O3XIJkJx"
      },
      "source": [
        "outputs_train = np.where(outputs_train >= 0.5 , 1 , outputs_train)\n",
        "outputs_train = np.where(outputs_train < 0.5 , 0 , outputs_train)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfpKdTr-J0_b",
        "outputId": "fc95702d-e224-4bd2-e6ae-41a8d7230750"
      },
      "source": [
        "(outputs_train == y_train).sum()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "360"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXwTbYzwKNcq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}