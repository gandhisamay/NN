# -*- coding: utf-8 -*-
"""NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K2i17WK3k4PJH_F83W495MZScO87fYn4
"""

import numpy as np 
import pandas as pd

from google.colab import drive 
drive.mount('drive')

!unzip '/content/drive/MyDrive/Datasets /archive (23).zip'

df = pd.read_csv('/content/Admission_Predict.csv')

pd.set_option('display.max_rows',400)

np.random.seed(10)

n_sample = df.shape[0]
shuffled_indices = np.random.permutation(n_sample)
#print(shuffled_indices)

features = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP',
       'LOR ', 'CGPA', 'Chance of Admit ']

x = np.array(df[features])
y = np.array(df['Research'])

n_val =int( 0.1 * df.shape[0])
print(n_val)

train_indices = shuffled_indices[n_val:]
test_indices = shuffled_indices[:n_val]

x_train = x[train_indices]
x_test  = x[test_indices]

y_train = y[train_indices]
y_test  = y[test_indices]

x_train

x_train = (x_train - x_train.mean(axis = 0))/x_train.std(axis = 0)
x_test = (x_test - x_test.mean(axis = 0))/x_test.std(axis = 0)

x_train = x_train.reshape(7,-1)
y_train = y_train.reshape(1,-1)
x_test = x_test.reshape(7,-1)
y_test = y_test.reshape(1,-1)

class NN:

  def __init__(self,input_size,h1_size,output_size):
    #-----Inputs are assigned to the model----#
    self.input_size = input_size
    self.h1_size = h1_size 
    self.output_size = output_size 
    
    #----Initiailze the weights----#
    self.W1 = np.random.uniform(low = -1,high = 1, size = (self.h1_size,self.input_size))
    self.b1 = np.zeros([self.h1_size,1])
    self.W2 = np.random.uniform(low = -1,high = 1, size = (self.output_size,self.h1_size))
    self.b2 = np.zeros([self.output_size,1])

  def forward(self,x):
    self.Z1 = np.dot(self.W1,x) + self.b1
    self.A1 = self.Tanh(self.Z1)

    self.Z2 = np.dot(self.W2,self.A1) + self.b2
    self.A2 = self.Sigmoid(self.Z2)
    return self.A2

  def NLLloss(self,output,y):
    loss = -np.sum((np.dot(y,np.log(self.A2).T) + np.dot(1-y,np.log(1-self.A2).T))/x.shape[1])
    return loss

  def backward(self,x,y):
    self.dA2 = (self.A2-y)/np.multiply(self.A2,1-self.A2)
    self.dZ2 = self.dA2*self.Sigmoid(self.Z2,derivative = True)
    self.dW2 = np.dot(self.dZ2,self.A1.T)/x.shape[1]
    self.db2 = np.sum(self.dZ2, axis = 1 , keepdims = True)/x.shape[1]

    self.dA1 = np.dot(self.W2.T,self.dZ2)
    self.dZ1 = self.dA1*self.Tanh(self.Z1, derivative = True)
    self.dW1 = np.dot(self.dZ1,x.T)/x.shape[1]
    self.db1 = np.sum(self.dZ1 , axis = 1 , keepdims = True)/x.shape[1]

  #-----Define various activation functions-----#
  def Sigmoid(self,x,derivative = False):
    if derivative == False:
      x = 1/(1 + np.exp(-x))
      return x
    else:
      y = 1/(1 + np.exp(-x))
      return y*(1-y)

  def Tanh(self,x,derivative = False):
    if derivative == False:
      x = np.tanh(x)
      return x
    else:
      y = np.tanh(x)
      return 1-y**2

model = NN(input_size=7,h1_size=64,output_size=1)

outputs_train = model.forward(x_train)
loss = model.NLLloss(outputs,y_train)
model.backward(x_train,y_train)

#------Update the parameters using Gradient Descent------#
lr = 1e-1
num_epochs = 500

for epochs in range(num_epochs):
  outputs = model.forward(x_train)
  loss = model.NLLloss(outputs,y_train)
  model.backward(x_train,y_train)
  #------Update the parameters------#
  model.W1 -= lr*model.dW1
  model.b1 -= lr*model.db1
  model.W2 -= lr*model.dW2
  model.b2 -= lr*model.db2
  #-----Print the loss-----#
  print("[{}/{}] Loss: {}".format(epochs+1,num_epochs,loss))

#----Model Accuracy on Test Dataset----#
outputs_test = model.forward(x_test)
loss = model.NLLloss(outputs,y_test)
print(loss)

outputs_test

outputs_train = np.where(outputs_train >= 0.5 , 1 , outputs_train)
outputs_train = np.where(outputs_train < 0.5 , 0 , outputs_train)

(outputs_train == y_train).sum()

